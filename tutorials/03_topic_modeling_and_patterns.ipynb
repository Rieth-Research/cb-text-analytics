{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Topic Modeling and Communication Patterns\n",
    "\n",
    "**Goal:** Discover actual topics and patterns in central bank communications using objective, measurable techniques.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Topic modeling with LDA (discover hidden themes)\n",
    "- Concrete keyword and term tracking\n",
    "- Language complexity metrics (objective readability)\n",
    "- Vocabulary diversity analysis\n",
    "- Communication pattern changes\n",
    "- Comparing language use across banks\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Why these methods?**\n",
    "These are **objective, measurable** techniques based on actual word usage and patterns, not subjective \"sentiment\" guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# NLP tools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Load data function\n",
    "def load_statements(directory, bank_name):\n",
    "    statements = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            date_str = filename.replace('.txt', '').replace('-txt', '')\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            statements.append({'date': date_str, 'bank': bank_name, 'text': text})\n",
    "    df = pd.DataFrame(statements)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "fed_data = load_statements('../usa-central-bank/fomc-statements', 'Fed')\n",
    "nz_data = load_statements('../nz-central-bank/ocr', 'RBNZ')\n",
    "all_data = pd.concat([fed_data, nz_data], ignore_index=True).sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(all_data)} statements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Topic Modeling with LDA\n",
    "\n",
    "**LDA (Latent Dirichlet Allocation)** discovers hidden topics automatically by finding words that tend to appear together.\n",
    "\n",
    "This is **objective** - it's based on actual statistical patterns in the text, not guessing emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text data\n",
    "# We'll use TF-IDF to weight important words\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=200,  # Top 200 most common words\n",
    "    stop_words='english',  # Remove common words\n",
    "    min_df=2,  # Word must appear in at least 2 documents\n",
    "    max_df=0.8  # Ignore words in more than 80% of documents\n",
    ")\n",
    "\n",
    "# Convert text to word counts\n",
    "doc_term_matrix = vectorizer.fit_transform(all_data['text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Created matrix: {doc_term_matrix.shape[0]} documents, {doc_term_matrix.shape[1]} words\")\n",
    "print(f\"\\nSample words: {', '.join(feature_names[:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LDA model to discover topics\n",
    "n_topics = 5  # We'll look for 5 main topics\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=20\n",
    ")\n",
    "\n",
    "# This finds the topics\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "print(\"âœ“ Topics discovered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topics\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    \"\"\"\n",
    "    Show the top words for each topic.\n",
    "    \"\"\"\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics[f\"Topic {topic_idx + 1}\"] = top_words\n",
    "    return topics\n",
    "\n",
    "topics = display_topics(lda, feature_names, n_top_words=10)\n",
    "\n",
    "print(\"Discovered Topics:\")\n",
    "print(\"=\" * 80)\n",
    "for topic_name, words in topics.items():\n",
    "    print(f\"\\n{topic_name}:\")\n",
    "    print(f\"  {', '.join(words)}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ These topics were discovered automatically from word co-occurrence patterns.\")\n",
    "print(\"   Can you identify what each topic is about? (e.g., inflation, employment, policy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Track Topic Prevalence Over Time\n",
    "\n",
    "Let's see which topics are emphasized in different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution for each document\n",
    "topic_distributions = lda.transform(doc_term_matrix)\n",
    "\n",
    "# Add to dataframe\n",
    "for i in range(n_topics):\n",
    "    all_data[f'topic_{i+1}'] = topic_distributions[:, i]\n",
    "\n",
    "# Plot topic prevalence over time for Fed\n",
    "fed_topics = all_data[all_data['bank'] == 'Fed']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i in range(n_topics):\n",
    "    plt.plot(fed_topics['date'], fed_topics[f'topic_{i+1}'], \n",
    "             label=f'Topic {i+1}', marker='o', linewidth=2)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Topic Weight', fontsize=12)\n",
    "plt.title('Topic Evolution in Fed Statements', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ This shows which topics were emphasized at different times.\")\n",
    "print(\"   Look for shifts that might correspond to economic events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Concrete Economic Term Tracking\n",
    "\n",
    "Instead of vague \"sentiment\", let's track **actual economic terms** and see what the bank is focusing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific economic terms to track\n",
    "economic_terms = {\n",
    "    'Inflation': ['inflation', 'price', 'prices'],\n",
    "    'Employment': ['employment', 'labor', 'jobs', 'unemployment'],\n",
    "    'Growth': ['growth', 'economic activity', 'expansion'],\n",
    "    'Risk': ['risk', 'uncertainty', 'uncertain'],\n",
    "    'Policy': ['policy', 'monetary policy', 'target'],\n",
    "    'Financial': ['financial', 'market', 'markets', 'credit']\n",
    "}\n",
    "\n",
    "def count_term_mentions(text, terms):\n",
    "    \"\"\"\n",
    "    Count mentions of terms (case-insensitive).\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    count = 0\n",
    "    for term in terms:\n",
    "        count += text_lower.count(term.lower())\n",
    "    return count\n",
    "\n",
    "# Count each category\n",
    "for category, terms in economic_terms.items():\n",
    "    all_data[category] = all_data['text'].apply(lambda x: count_term_mentions(x, terms))\n",
    "\n",
    "# Normalize by document length\n",
    "all_data['word_count'] = all_data['text'].str.split().str.len()\n",
    "for category in economic_terms.keys():\n",
    "    all_data[f'{category}_per_100'] = (all_data[category] / all_data['word_count']) * 100\n",
    "\n",
    "print(\"âœ“ Term tracking complete\")\n",
    "all_data[['date', 'bank'] + [f'{cat}_per_100' for cat in economic_terms.keys()]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot term frequency over time\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, category in enumerate(economic_terms.keys()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for bank in all_data['bank'].unique():\n",
    "        bank_data = all_data[all_data['bank'] == bank]\n",
    "        ax.plot(bank_data['date'], bank_data[f'{category}_per_100'], \n",
    "               marker='o', label=bank, linewidth=2, markersize=4)\n",
    "    \n",
    "    ax.set_title(f'{category} Mentions', fontweight='bold')\n",
    "    ax.set_ylabel('Mentions per 100 words')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ This shows concrete, measurable changes in what central banks talk about.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Language Complexity Analysis\n",
    "\n",
    "Measure **objective** readability metrics. Are statements getting simpler or more complex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complexity_metrics(text):\n",
    "    \"\"\"\n",
    "    Calculate objective text complexity metrics.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s for s in sentences if s.strip()]  # Remove empty\n",
    "    \n",
    "    if len(words) == 0 or len(sentences) == 0:\n",
    "        return {'avg_sentence_length': 0, 'avg_word_length': 0, 'long_words_pct': 0}\n",
    "    \n",
    "    # Average sentence length (words per sentence)\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    \n",
    "    # Average word length (characters)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    # Percentage of \"long\" words (7+ characters)\n",
    "    long_words = sum(1 for word in words if len(word) >= 7)\n",
    "    long_words_pct = (long_words / len(words)) * 100\n",
    "    \n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'long_words_pct': long_words_pct\n",
    "    }\n",
    "\n",
    "# Calculate for all documents\n",
    "complexity = all_data['text'].apply(calculate_complexity_metrics)\n",
    "all_data['avg_sentence_length'] = complexity.apply(lambda x: x['avg_sentence_length'])\n",
    "all_data['avg_word_length'] = complexity.apply(lambda x: x['avg_word_length'])\n",
    "all_data['long_words_pct'] = complexity.apply(lambda x: x['long_words_pct'])\n",
    "\n",
    "print(\"âœ“ Complexity metrics calculated\")\n",
    "all_data[['date', 'bank', 'avg_sentence_length', 'avg_word_length', 'long_words_pct']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot complexity over time\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = [\n",
    "    ('avg_sentence_length', 'Average Sentence Length (words)'),\n",
    "    ('avg_word_length', 'Average Word Length (characters)'),\n",
    "    ('long_words_pct', 'Long Words (%)')\n",
    "]\n",
    "\n",
    "for idx, (metric, title) in enumerate(metrics):\n",
    "    for bank in all_data['bank'].unique():\n",
    "        bank_data = all_data[all_data['bank'] == bank]\n",
    "        axes[idx].plot(bank_data['date'], bank_data[metric], \n",
    "                      marker='o', label=bank, linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(title, fontweight='bold')\n",
    "    axes[idx].set_ylabel(metric.replace('_', ' ').title())\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Increasing values = more complex language.\")\n",
    "print(\"   This is objective and measurable, unlike sentiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Vocabulary Diversity\n",
    "\n",
    "Are statements using more varied language, or repeating the same words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vocabulary_diversity(text):\n",
    "    \"\"\"\n",
    "    Calculate lexical diversity (Type-Token Ratio).\n",
    "    Higher = more diverse vocabulary.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    unique_words = len(set(words))\n",
    "    total_words = len(words)\n",
    "    \n",
    "    return unique_words / total_words\n",
    "\n",
    "all_data['vocab_diversity'] = all_data['text'].apply(calculate_vocabulary_diversity)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "for bank in all_data['bank'].unique():\n",
    "    bank_data = all_data[all_data['bank'] == bank]\n",
    "    plt.plot(bank_data['date'], bank_data['vocab_diversity'], \n",
    "            marker='o', label=bank, linewidth=2)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Vocabulary Diversity (Type-Token Ratio)', fontsize=12)\n",
    "plt.title('Language Diversity Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Higher diversity = more varied language (less repetitive).\")\n",
    "print(\"   Lower diversity = more standardized, formulaic language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Most Distinctive Words by Bank\n",
    "\n",
    "What words are characteristic of each bank? (Using TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare banks using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine all text for each bank\n",
    "bank_texts = all_data.groupby('bank')['text'].apply(lambda x: ' '.join(x)).to_dict()\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(bank_texts.values())\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Get top words for each bank\n",
    "for idx, bank in enumerate(bank_texts.keys()):\n",
    "    scores = tfidf_matrix[idx].toarray()[0]\n",
    "    top_indices = scores.argsort()[-15:][::-1]\n",
    "    top_words = [(feature_names[i], scores[i]) for i in top_indices]\n",
    "    \n",
    "    print(f\"\\nMost Distinctive Words for {bank}:\")\n",
    "    print(\"=\" * 60)\n",
    "    for word, score in top_words[:10]:\n",
    "        print(f\"  {word:20s} {score:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ These words are statistically distinctive to each bank's language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Changing Language Patterns\n",
    "\n",
    "Detect when language patterns shift significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate change in key metrics\n",
    "for bank in all_data['bank'].unique():\n",
    "    mask = all_data['bank'] == bank\n",
    "    \n",
    "    # Calculate change from previous statement\n",
    "    all_data.loc[mask, 'length_change'] = all_data.loc[mask, 'word_count'].diff()\n",
    "    all_data.loc[mask, 'complexity_change'] = all_data.loc[mask, 'avg_sentence_length'].diff()\n",
    "    all_data.loc[mask, 'diversity_change'] = all_data.loc[mask, 'vocab_diversity'].diff()\n",
    "\n",
    "# Find biggest changes\n",
    "print(\"Biggest Length Increases:\")\n",
    "print(all_data.nlargest(5, 'length_change')[['date', 'bank', 'word_count', 'length_change']])\n",
    "\n",
    "print(\"\\nBiggest Complexity Increases:\")\n",
    "print(all_data.nlargest(5, 'complexity_change')[['date', 'bank', 'avg_sentence_length', 'complexity_change']])\n",
    "\n",
    "print(\"\\nðŸ’¡ These dates might mark important communication shifts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary Dashboard\n",
    "\n",
    "Bring it all together in one comprehensive view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Topic evolution (Fed only for clarity)\n",
    "fed_topics = all_data[all_data['bank'] == 'Fed']\n",
    "for i in range(min(3, n_topics)):  # Show top 3 topics\n",
    "    axes[0, 0].plot(fed_topics['date'], fed_topics[f'topic_{i+1}'], \n",
    "                   label=f'Topic {i+1}', marker='o', linewidth=2)\n",
    "axes[0, 0].set_title('Topic Evolution (Fed)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Topic Weight')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Inflation mentions\n",
    "for bank in all_data['bank'].unique():\n",
    "    bank_data = all_data[all_data['bank'] == bank]\n",
    "    axes[0, 1].plot(bank_data['date'], bank_data['Inflation_per_100'], \n",
    "                   marker='o', label=bank, linewidth=2)\n",
    "axes[0, 1].set_title('Inflation Mentions per 100 Words', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Mentions')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Language complexity\n",
    "for bank in all_data['bank'].unique():\n",
    "    bank_data = all_data[all_data['bank'] == bank]\n",
    "    axes[1, 0].plot(bank_data['date'], bank_data['avg_sentence_length'], \n",
    "                   marker='o', label=bank, linewidth=2)\n",
    "axes[1, 0].set_title('Sentence Length (Complexity)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Words per Sentence')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Vocabulary diversity\n",
    "for bank in all_data['bank'].unique():\n",
    "    bank_data = all_data[all_data['bank'] == bank]\n",
    "    axes[1, 1].plot(bank_data['date'], bank_data['vocab_diversity'], \n",
    "                   marker='o', label=bank, linewidth=2)\n",
    "axes[1, 1].set_title('Vocabulary Diversity', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Type-Token Ratio')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Central Bank Communication Analysis Dashboard', \n",
    "            fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What You Learned\n",
    "\n",
    "1. **Topic Modeling (LDA)**: Discover hidden themes statistically\n",
    "2. **Concrete Term Tracking**: Measure actual economic focus areas\n",
    "3. **Complexity Metrics**: Objective readability measurements\n",
    "4. **Vocabulary Diversity**: Measure language variation\n",
    "5. **TF-IDF Analysis**: Find distinctive words\n",
    "6. **Change Detection**: Identify significant shifts\n",
    "\n",
    "**Key Difference from Sentiment Analysis:**\n",
    "- These are **objective, measurable** metrics\n",
    "- Based on **actual word usage and patterns**\n",
    "- Not trying to guess \"emotions\" from formal documents\n",
    "- More appropriate for analyzing technical communications\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "In Tutorial 4, we'll learn:\n",
    "- Creating publication-ready visualizations\n",
    "- Interactive dashboards\n",
    "- Exporting results\n",
    "\n",
    "## ðŸ’¡ Try It Yourself\n",
    "\n",
    "1. Experiment with different numbers of topics in LDA\n",
    "2. Add more economic terms to track\n",
    "3. Compare early vs late periods for each bank\n",
    "4. Find correlations between metrics (e.g., complexity vs diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
