{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Text Analysis Basics\n",
    "\n",
    "**Goal:** Learn fundamental text analysis techniques to extract insights from central bank statements.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Text preprocessing and cleaning\n",
    "- Word frequency analysis\n",
    "- Finding important keywords\n",
    "- Basic text statistics\n",
    "- Simple visualizations\n",
    "\n",
    "**Time:** ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Load Data\n",
    "\n",
    "We'll reuse the loading function from Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re  # Regular expressions for text cleaning\n",
    "\n",
    "# Make plots show up in the notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Function from Tutorial 1\n",
    "def load_statements(directory, bank_name):\n",
    "    statements = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            date_str = filename.replace('.txt', '').replace('-txt', '')\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            statements.append({\n",
    "                'date': date_str,\n",
    "                'bank': bank_name,\n",
    "                'text': text,\n",
    "                'filename': filename\n",
    "            })\n",
    "    df = pd.DataFrame(statements)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "fed_data = load_statements('../usa-central-bank/fomc-statements', 'Fed')\n",
    "print(f\"âœ“ Loaded {len(fed_data)} Fed statements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text Preprocessing\n",
    "\n",
    "**Why clean text?** Raw text has inconsistencies. We need to:\n",
    "- Convert to lowercase (so \"Inflation\" and \"inflation\" are treated the same)\n",
    "- Remove punctuation\n",
    "- Remove common words (\"the\", \"a\", \"and\") called \"stop words\"\n",
    "\n",
    "This makes analysis more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text for analysis.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove punctuation and special characters\n",
    "    3. Split into words\n",
    "    \n",
    "    Returns: list of cleaned words\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and keep only letters and spaces\n",
    "    # \\W+ means \"one or more non-word characters\"\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    \n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Test it\n",
    "sample = \"The Committee decided to maintain the target range for the federal funds rate at 0 to 1/4 percent.\"\n",
    "print(\"Original:\")\n",
    "print(sample)\n",
    "print(\"\\nCleaned:\")\n",
    "print(clean_text(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stop Words\n",
    "\n",
    "**Stop words** are common words that don't carry much meaning: \"the\", \"a\", \"is\", etc.\n",
    "\n",
    "We remove them to focus on meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common English stop words\n",
    "STOP_WORDS = set([\n",
    "    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',\n",
    "    'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
    "    'should', 'could', 'may', 'might', 'must', 'can', 'this', 'that',\n",
    "    'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "    'their', 'our', 'your', 'its'\n",
    "])\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    \"\"\"\n",
    "    Remove common stop words from a list of words.\n",
    "    \"\"\"\n",
    "    return [word for word in words if word not in STOP_WORDS and len(word) > 2]\n",
    "\n",
    "# Test it\n",
    "test_words = ['the', 'committee', 'decided', 'to', 'maintain', 'the', 'rate']\n",
    "print(\"Before:\", test_words)\n",
    "print(\"After:\", remove_stop_words(test_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Word Frequency Analysis\n",
    "\n",
    "Let's find the most common words in Fed statements. This shows what topics they talk about most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequencies(df, n=20):\n",
    "    \"\"\"\n",
    "    Get the most common words across all statements.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with 'text' column\n",
    "    - n: number of top words to return\n",
    "    \n",
    "    Returns: list of (word, count) tuples\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    \n",
    "    # Process each statement\n",
    "    for text in df['text']:\n",
    "        words = clean_text(text)\n",
    "        words = remove_stop_words(words)\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count frequencies using Counter (like a smart dictionary)\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    # Get top N most common\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "# Get top 20 words\n",
    "top_words = get_word_frequencies(fed_data, n=20)\n",
    "\n",
    "print(\"Top 20 Most Common Words in Fed Statements:\")\n",
    "print(\"=\" * 50)\n",
    "for word, count in top_words:\n",
    "    print(f\"{word:20s} {count:5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Word Frequencies\n",
    "\n",
    "A picture is worth a thousand words! Let's create a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "words = [word for word, count in top_words[:15]]  # Top 15 for readability\n",
    "counts = [count for word, count in top_words[:15]]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(words, counts, color='steelblue')\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Most Common Words in Fed Statements (2014-2017)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ What do these words tell us?\")\n",
    "print(\"   - 'committee' appears most (that's who issues the statements)\")\n",
    "print(\"   - Economic terms like 'economic', 'employment', 'inflation' are common\")\n",
    "print(\"   - This shows the key topics the Fed focuses on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Track Keywords Over Time\n",
    "\n",
    "Let's see how often specific keywords appear over time. This shows shifting priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keyword(text, keyword):\n",
    "    \"\"\"\n",
    "    Count how many times a keyword appears in text (case-insensitive).\n",
    "    \"\"\"\n",
    "    return text.lower().count(keyword.lower())\n",
    "\n",
    "# Keywords we care about\n",
    "keywords = ['inflation', 'employment', 'growth', 'risk']\n",
    "\n",
    "# Count each keyword in each statement\n",
    "for keyword in keywords:\n",
    "    fed_data[keyword] = fed_data['text'].apply(lambda x: count_keyword(x, keyword))\n",
    "\n",
    "# Show first few rows\n",
    "fed_data[['date', 'inflation', 'employment', 'growth', 'risk']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot keyword trends over time\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for keyword in keywords:\n",
    "    plt.plot(fed_data['date'], fed_data[keyword], marker='o', label=keyword.title(), linewidth=2)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Mentions per Statement', fontsize=12)\n",
    "plt.title('Keyword Frequency in Fed Statements Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insights:\")\n",
    "print(\"   - You can see when certain topics become more/less important\")\n",
    "print(\"   - Spikes might correlate with economic events\")\n",
    "print(\"   - This is how researchers track changing priorities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Bigrams (Two-Word Phrases)\n",
    "\n",
    "Sometimes two words together have more meaning than alone.\n",
    "Examples: \"interest rate\", \"economic growth\", \"labor market\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text, n=15):\n",
    "    \"\"\"\n",
    "    Find most common two-word phrases.\n",
    "    \"\"\"\n",
    "    words = clean_text(text)\n",
    "    \n",
    "    # Create pairs of consecutive words\n",
    "    bigrams = []\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = f\"{words[i]} {words[i+1]}\"\n",
    "        bigrams.append(bigram)\n",
    "    \n",
    "    # Count and return top N\n",
    "    return Counter(bigrams).most_common(n)\n",
    "\n",
    "# Get all text combined\n",
    "all_text = ' '.join(fed_data['text'])\n",
    "\n",
    "top_bigrams = get_bigrams(all_text, n=15)\n",
    "\n",
    "print(\"Top 15 Two-Word Phrases:\")\n",
    "print(\"=\" * 50)\n",
    "for phrase, count in top_bigrams:\n",
    "    print(f\"{phrase:30s} {count:5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Time Periods\n",
    "\n",
    "Let's compare early vs. late statements to see how language evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in half by date\n",
    "mid_point = len(fed_data) // 2\n",
    "early = fed_data.iloc[:mid_point]\n",
    "late = fed_data.iloc[mid_point:]\n",
    "\n",
    "print(f\"Early period: {early['date'].min().date()} to {early['date'].max().date()}\")\n",
    "print(f\"Late period: {late['date'].min().date()} to {late['date'].max().date()}\")\n",
    "\n",
    "# Get top words for each period\n",
    "early_words = dict(get_word_frequencies(early, n=10))\n",
    "late_words = dict(get_word_frequencies(late, n=10))\n",
    "\n",
    "print(\"\\nTop 10 Words - Early Period:\")\n",
    "for word, count in list(early_words.items())[:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Words - Late Period:\")\n",
    "for word, count in list(late_words.items())[:10]:\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Text Readability\n",
    "\n",
    "Let's measure how complex the statements are. We'll use **Flesch Reading Ease**:\n",
    "- Higher score = easier to read\n",
    "- 60-70 = standard writing\n",
    "- 0-30 = very difficult (college level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability(text):\n",
    "    \"\"\"\n",
    "    Calculate Flesch Reading Ease score.\n",
    "    Formula: 206.835 - 1.015 * (words/sentences) - 84.6 * (syllables/words)\n",
    "    \n",
    "    Simplified version using word and sentence counts.\n",
    "    \"\"\"\n",
    "    words = len(text.split())\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    \n",
    "    if sentences == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Simplified calculation (without syllable counting)\n",
    "    avg_sentence_length = words / sentences\n",
    "    \n",
    "    # Estimate complexity based on sentence length\n",
    "    # Longer sentences = harder to read\n",
    "    score = 100 - (avg_sentence_length * 2)\n",
    "    \n",
    "    return max(0, score)  # Keep score above 0\n",
    "\n",
    "# Calculate for all statements\n",
    "fed_data['readability'] = fed_data['text'].apply(calculate_readability)\n",
    "\n",
    "# Plot over time\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(fed_data['date'], fed_data['readability'], marker='o', linewidth=2, color='purple')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Readability Score', fontsize=12)\n",
    "plt.title('Fed Statement Readability Over Time (Higher = Easier)', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=60, color='red', linestyle='--', label='Standard writing')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage readability score: {fed_data['readability'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What You Learned\n",
    "\n",
    "1. **Text cleaning**: Lowercase, punctuation removal, stop words\n",
    "2. **Word frequency**: Finding most common words with Counter\n",
    "3. **Visualization**: Creating bar charts and line plots\n",
    "4. **Time series analysis**: Tracking keywords over time\n",
    "5. **Bigrams**: Finding meaningful phrases\n",
    "6. **Readability**: Measuring text complexity\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "In Tutorial 3, we'll learn:\n",
    "- Sentiment analysis (is the tone positive/negative?)\n",
    "- Hawkish vs dovish language detection\n",
    "- Advanced NLP with specialized libraries\n",
    "\n",
    "## ðŸ’¡ Try It Yourself\n",
    "\n",
    "1. Load RBNZ data and compare top words with Fed\n",
    "2. Add more keywords to track (try: \"uncertainty\", \"policy\", \"committee\")\n",
    "3. Find the most readable and least readable statements\n",
    "4. Create a word cloud (requires `wordcloud` library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
